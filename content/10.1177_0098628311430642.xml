<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="review-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">TOP</journal-id>
<journal-id journal-id-type="hwp">sptop</journal-id>
<journal-title>Teaching of Psychology</journal-title>
<issn pub-type="ppub">0098-6283</issn>
<issn pub-type="epub">1532-2802</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0098628311430642</article-id>
<article-id pub-id-type="publisher-id">10.1177_0098628311430642</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Topical Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Guidelines for a Scientific Approach to Critical Thinking Assessment</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Bensley</surname>
<given-names>D. Alan</given-names>
</name>
<xref ref-type="aff" rid="aff1-0098628311430642">1</xref>
<xref ref-type="corresp" rid="corresp1-0098628311430642"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Murtagh</surname>
<given-names>Michael P.</given-names>
</name>
<xref ref-type="aff" rid="aff1-0098628311430642">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-0098628311430642">
<label>1</label>Frostburg State University, Frostburg, MD, USA</aff>
<author-notes>
<corresp id="corresp1-0098628311430642">D. Alan Bensley, Department of Psychology, Frostburg State University, Frostburg, MD 21532 Email: <email>abensley@frostburg.edu</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>1</month>
<year>2012</year>
</pub-date>
<volume>39</volume>
<issue>1</issue>
<fpage>5</fpage>
<lpage>16</lpage>
<permissions>
<copyright-statement>© Society for the Teaching of Psychology 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">Society for the Teaching of Psychology</copyright-holder>
</permissions>
<abstract>
<p>Assessment of student learning outcomes can be a powerful tool for improvement of instruction when a scientific approach is taken; unfortunately, many educators do not take full advantage of this approach. This article examines benefits of taking a scientific approach to critical thinking assessment and proposes guidelines for planning, conducting, and using assessment research. Specifically, we discuss study design options and strategies for improving the quality of assessment data and the use of such data to improve critical thinking instruction in programs. Examples of practices from three programs that have conducted considerable critical thinking assessment illustrate use of the guidelines. Using scientific assessment to develop evidence-based educational practices shows great promise in contributing to the scholarship of teaching critical thinking.</p>
</abstract>
<kwd-group>
<kwd>assessment</kwd>
<kwd>critical thinking</kwd>
<kwd>instruction</kwd>
<kwd>learning outcomes</kwd>
<kwd>psychology programs</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Many educators, even some psychologists, grimace at the mere mention of learning outcomes assessment (LOA), viewing it as an odious chore that must be completed to satisfy the requirements of their institution or accrediting body. The reluctance of psychologists to assess the critical thinking (CT) of their students seems particularly ironic given that so many endorse CT as an outcome and are otherwise eager to scientifically study the thinking of students, their typical research participants. It also seems surprising given the considerable attention of the field to promoting the science and scholarship of pedagogy (e.g., <xref ref-type="bibr" rid="bibr41-0098628311430642">Halpern et al., 1998</xref>; <xref ref-type="bibr" rid="bibr64-0098628311430642">Smith &amp; Buskist, 2008</xref>).</p>
<p>The purpose of this article is to facilitate LOA research by offering practical guidelines for collecting high-quality LOA data that can provide a scientific basis for improving CT instruction. We have based our suggestions on best practices supported by scientific research and examples from three undergraduate psychology programs, Alverno College (AC), Frostburg State University (FSU), and James Madison University (JMU). These three programs not only have done considerable CT assessment, but also have taken very different approaches. CT remains controversial as a construct (e.g., <xref ref-type="bibr" rid="bibr10-0098628311430642">Bensley, 2009</xref>, <xref ref-type="bibr" rid="bibr12-0098628311430642">in press</xref>; <xref ref-type="bibr" rid="bibr78-0098628311430642">Yanchar, Slife, &amp; Warne, 2008</xref>); and we hope that in addition to illustrating the guidelines and options available, comparing different approaches will promote discussion of how to best conduct scientific LOA.</p>
<p>Our main thesis is that LOA in general, and CT assessment in particular, should be conducted in the same manner as any good scientific research. Among academics, psychologists are in a unique position to take a scientific approach to LOA because they often have had training relevant to conducting such research (<xref ref-type="bibr" rid="bibr69-0098628311430642">Stoloff, Apple, Barron, Reis-Bergan, &amp; Sundre, 2004</xref>). Those collecting LOA data should use relevant knowledge of psychological research, testing, psychometrics, and methodology to collect high-quality data. When possible, they should conduct appropriate quantitative analyses of their data. Assessment with qualitative data can, of course, be scientific; however, those using qualitative methods such as rubrics often miss opportunities to systematically measure and quantitatively analyze their data, which would increase the ability to understand and predict learning outcomes.</p>
<p>Instructors often overlook other benefits of taking a scientific approach to LOA. The classroom is a natural laboratory with research participants readily accessible for instructors to do assessment research, affording the opportunity to test the effectiveness of instructional strategies and programs hypothesized to improve student learning and thinking. Although assessment is often viewed as a local knowledge-gathering enterprise, theoretically motivated assessment research can inform psychologists outside of the local assessment setting. For example, a longitudinal LOA study by <xref ref-type="bibr" rid="bibr46-0098628311430642">Lehman and Nisbett (1990)</xref> added to the knowledge base of psychology by showing that the psychology majors produced greater gains in statistical and methodological reasoning than natural science and humanities majors, who showed more improvement in conditional reasoning, suggesting differences in CT skills across disciplines.</p>
<sec id="section1-0098628311430642">
<title>Guidelines for Effective Assessment</title>
<p>To help researchers and instructors accrue the benefits of taking a scientific approach to LOA, we propose eight guidelines described below. These guidelines are not an exhaustive list of good practices intended to replace the useful suggestions of others (e.g., <xref ref-type="bibr" rid="bibr2-0098628311430642">Allen, 2004</xref>; <xref ref-type="bibr" rid="bibr7-0098628311430642">Angelo &amp; Cross, 1993</xref>; <xref ref-type="bibr" rid="bibr34-0098628311430642">Graham, 1998</xref>; <xref ref-type="bibr" rid="bibr36-0098628311430642">Halonen, 2008</xref>; <xref ref-type="bibr" rid="bibr38-0098628311430642">Halpern, 1988</xref>, <xref ref-type="bibr" rid="bibr40-0098628311430642">2004</xref>; <xref ref-type="bibr" rid="bibr50-0098628311430642">Maki, 2004</xref>; <xref ref-type="bibr" rid="bibr60-0098628311430642">Pieper, Fulcher, Sundre, &amp; Erwin, 2008</xref>; <xref ref-type="bibr" rid="bibr62-0098628311430642">Pusateri, Halonen, Hill, &amp; McCarthy, 2009</xref>; <xref ref-type="bibr" rid="bibr69-0098628311430642">Stoloff et al., 2004</xref>). Rather, the following eight guidelines are intended to provide practical suggestions for conducting high-quality assessment research on CT in psychology and for using assessment results to improve CT instruction.</p>
<sec id="section2-0098628311430642">
<title>1. Understand Critical Thinking as a Multidimensional Construct</title>
<p>As with other research, it is difficult to conduct LOA research on CT without first understanding the underlying construct. Problems and disagreements in conceptualizing CT have posed serious challenges to its assessment (<xref ref-type="bibr" rid="bibr12-0098628311430642">Bensley, in press</xref>; <xref ref-type="bibr" rid="bibr35-0098628311430642">Halonen, 1995</xref>). Despite the disagreements, many experts agree that CT involves both skills and dispositions related to reasoning (e.g., <xref ref-type="bibr" rid="bibr29-0098628311430642">Ennis, 1987</xref>; <xref ref-type="bibr" rid="bibr39-0098628311430642">Halpern, 1998</xref>); and research has supported this (<xref ref-type="bibr" rid="bibr27-0098628311430642">Clifford, Boufal, &amp; Kurtz, 2004</xref>; <xref ref-type="bibr" rid="bibr72-0098628311430642">Taube, 1997</xref>). Examples of CT skills used in psychology are skills for argument analysis and evaluation, methodological reasoning, statistical reasoning, causal reasoning, and for focusing and clarifying questions. Examples of CT dispositions are the willingness to engage in effortful thinking and the tendency to be open- and fair-minded in evaluating claims, yet remain skeptical of unsubstantiated claims. It is important to assess both CT skills and dispositions because a person with the requisite CT skills may still not think critically if lacking the disposition, and vice versa.</p>
<p>Critical thinkers are also often assumed to be self-regulated thinkers who use CT to guide their behavior and develop their beliefs (<xref ref-type="bibr" rid="bibr39-0098628311430642">Halpern, 1998</xref>). This third psychological aspect of CT is related to metacognition, which refers to knowledge, awareness, and control of one’s own thinking. Although someone may have CT skills and be disposed to use them, that person will be less likely to use the skills appropriately if unaware of when to use them or if lacking knowledge for how to deploy them in a particular situation. It should be further noted that motivation contributes to self-regulation and CT (<xref ref-type="bibr" rid="bibr12-0098628311430642">Bensley, in press</xref>; <xref ref-type="bibr" rid="bibr71-0098628311430642">Tarricone, 2011</xref>).</p>
<p>This brief overview suggests that CT is a multidimensional construct, and assessments of it should include measures of skills, dispositions, and metacognition. Only a few studies have empirically studied both CT skills and dispositions (e.g., <xref ref-type="bibr" rid="bibr27-0098628311430642">Clifford et al., 2004</xref>; <xref ref-type="bibr" rid="bibr72-0098628311430642">Taube, 1997</xref>), and even fewer have examined both in the same assessment research study of psychology students (e.g., <xref ref-type="bibr" rid="bibr17-0098628311430642">Bensley, Spero, Kennedy, Murtagh, &amp; Bernhardt, 2010</xref>; <xref ref-type="bibr" rid="bibr66-0098628311430642">Spero, Bensley, &amp; Kennedy, 2010</xref>). None have thoroughly examined the skills, dispositions, and metacognitive aspects of CT in the same study.</p>
<p>Of particular relevance to the question of how to conceptualize and assess CT is whether knowledge and rules for thinking critically are general or discipline-specific. Although often treated as a general ability that improves through college instruction (<xref ref-type="bibr" rid="bibr58-0098628311430642">Pascarella &amp; Terrenzini, 1991</xref>), others have argued that CT is specific to disciplinary content and rules (<xref ref-type="bibr" rid="bibr53-0098628311430642">McPeck, 1981</xref>). Research suggests that some reasoning skills may be general whereas others are specific to training in a discipline (<xref ref-type="bibr" rid="bibr46-0098628311430642">Lehman &amp; Nisbett, 1990</xref>) and that CT may have limited generalizability (<xref ref-type="bibr" rid="bibr74-0098628311430642">Toplak &amp; Stanovich, 2002</xref>). LOA researchers should be aware that demonstrating proficiency in psychological CT does not necessarily imply that students have general CT skills, and vice versa. Both general and psychology-specific CT skills may need to be targeted for instruction and assessment.</p>
</sec>
<sec id="section3-0098628311430642">
<title>2. Select Important Goals, Objectives, and Outcomes for Assessment</title>
<p>It is important to first ask what are the CT skills, knowledge, and dispositions that students should be able to demonstrate as a result of being in a course or program. A useful resource for guiding this process is the document recommending a comprehensive set of goals, objectives, and outcomes written by an American Psychological Association Task Force on Undergraduate Major Competencies (<xref ref-type="bibr" rid="bibr4-0098628311430642">APA Task Force; 2002</xref>). One of the document’s 10 major goal categories is devoted entirely to CT. The CT category lists 16 suggested outcomes; however, other categories such as the research methods category have outcomes overlapping with CT.</p>
<p>A useful second step is to identify courses in the curriculum thought to address CT. One systematic way to achieve this is to construct a curriculum alignment matrix (<xref ref-type="bibr" rid="bibr2-0098628311430642">Allen, 2004</xref>). An example of a curriculum alignment matrix for a psychology program can be viewed in <xref ref-type="bibr" rid="bibr48-0098628311430642">Levy, Burton, Mickler, and Vigorito (1999)</xref>. Alignment refers to the degree of relationship between what students do in their courses and what faculty members expect them to learn. For example, using the <xref ref-type="bibr" rid="bibr4-0098628311430642">APA Task Force’s list (2002)</xref>, the FSU Psychology Department mapped specific goals to courses. Because FSU has historically emphasized the acquisition of CT and research methods skills, faculty members expected CT to be an outcome in many courses, especially in the research methods course sequence. For a complex goal such as improving CT, it may be necessary to further analyze what components of CT are expected to improve in particular courses and also whether multiple objectives may be addressed by a single course. By assessing its research methods students on methodological and causal reasoning about research (<xref ref-type="bibr" rid="bibr19-0098628311430642">Bernhardt, Foley, &amp; Evans, 2010</xref>) and on argument analysis skills (<xref ref-type="bibr" rid="bibr14-0098628311430642">Bensley, Crowe, Bernhardt, Buckner, &amp; Allman, 2010</xref>), FSU has addressed goals for both research methods and CT.</p>
<p>Some programs, such as the AC Psychology Department, have achieved great coherence in their assessment of goals, identifying five general program outcomes from the set of eight abilities developed by the college at large. Three goals related to CT are “Theoretical Reasoning: To construct plausible explanations for complex human behavior relying on reasoning and evidence”; “Methodological Proficiency: To solve problems by selecting, using, and evaluating a variety of psychological methods”; and “Self Reflection: To be self-directed in using the tools of metacognition to accurately discern the quality of her own performance in academic, professional, personal, and civic settings” (K. Vasquez, personal communication, July 22, 2010). Taking a developmental approach, AC has established outcomes and criteria for lower and upper level courses. The upper level outcomes include three especially related to CT: theoretical reasoning, methodological proficiency, and self-reflection. In this way, they have developed a comprehensive assessment plan and assessments that are further coordinated with those in other AC departments, lending coherence across the college’s curriculum. Like AC, JMU has received strong institutional support for assessment, which has helped them develop a comprehensive approach within a culture of assessment. Smaller departments with less support need to strategically focus on the most important goals and outcomes they have identified.</p>
<p>The conduct of scientific assessment research requires that goals, objectives, and outcomes be translated into specific hypotheses that are testable and falsifiable. Some hypotheses based on goals may be so general they need further specification to be very informative about the impact of instruction. Other outcomes suggested by the <xref ref-type="bibr" rid="bibr4-0098628311430642">APA Task Force (2002)</xref> can be more readily translated into specific LOA hypotheses—such as the prediction that after instruction in research report writing, students should be able to “develop sound arguments based on reasoning and evidence” (p. 13).</p>
</sec>
<sec id="section4-0098628311430642">
<title>3. Align Assessment With Instructional Focus</title>
<p>Instructors often assume that important educational objectives, such as the improvement of CT skills, are already being reached in their courses. This assumption can be tested when assessments have been aligned with instructional focus. Measures for assessing the impact of instruction must be sensitive to the changes instruction is intended to produce. Instructors should identify specific outcomes related to their instructional focus and CT theory that are likely to be observed in assessment.</p>
<p>One implication of this discussion is that classroom assessment can be an important tool for evaluating instructional techniques already being used and for comparing them with potentially better techniques. Both the AC and FSU psychology departments have made extensive use of classroom assessment to evaluate learning outcomes within classrooms in their programs. In addition, when summative classroom assessments are conducted at strategic points in the curriculum, they can contribute important process information to program assessment that is not available when assessment is conducted only at the start and completion of a program.</p>
<p>Moreover, programs have courses that are assumed to be the vehicles for certain learning events to occur, such as courses expected to promote CT. Assessments should be conducted in those courses and aligned with instruction. For example, <xref ref-type="bibr" rid="bibr59-0098628311430642">Penningroth, Despain, and Gray (2007)</xref> reported on a psychological problem-solving course early in their curriculum expected to improve CT skills for evaluating the quality of research. They pre- and posttested a class on problem solving in psychology using the psychological critical thinking test of <xref ref-type="bibr" rid="bibr44-0098628311430642">Lawson (1999)</xref>, a measure likely to be sensitive to acquisition of those skills. Because <xref ref-type="bibr" rid="bibr59-0098628311430642">Penningroth et al. (2007)</xref> used a test sensitive to skills for analyzing psychological research and information, they could show that their explicit instruction produced changes on this test although content of instructional materials and test questions differed.</p>
<p>At the lower level of lesson and assessment design, <xref ref-type="bibr" rid="bibr12-0098628311430642">Bensley (in press)</xref> has proposed an approach for aligning CT instruction with assessment. This approach views CT as the appropriate use of normative rules for reasoning in a discourse context. For example, rules for evaluating the quality of evidence used in psychological discussions can be infused into instruction by way of materials that require students to appropriately apply the rules to analyze a literature review to arrive at a sound inductive conclusion or make some other judgment. Formative and summative assessments can be aligned with instruction by creating (a) other discourse samples in the form of exercises, quizzes, and tests employing other questions and (b) literature reviews in which students must appropriately apply the relevant rules of reasoning. In this way, CT instruction can be aligned with assessment using discourse samples related to specific tasks in the discipline.</p>
</sec>
<sec id="section5-0098628311430642">
<title>4. Take an Authentic Task-Oriented Approach to Assessment</title>
<p>Given that working in psychology requires effective thinking and task execution, students should be taught how to think about psychological tasks and be assessed on their ability to perform such tasks. Examples of psychological tasks requiring CT include evaluating the quality of information from the Internet, analyzing and evaluating the research literature, using psychological theory to analyze and evaluate behavior, writing research and case reports, making differential diagnoses of mental disorders, deciding on the most effective treatment for a disorder, and applying an appropriate treatment.</p>
<p>LOA researchers taking an authentic task-oriented assessment approach often use performance assessment (PA) to assess how well students perform a task, requiring that they access relevant knowledge and skills to perform some meaningful, discipline-specific task. PA can also be used to assess how well people think, work, and participate in everyday life after college (<xref ref-type="bibr" rid="bibr42-0098628311430642">Janesick, 2006</xref>). An example of an authentic task from the AC Psychology Department is an assignment in which students take a 3-year-old child to lunch and then analyze the behavior of the child using theories of child development (<xref ref-type="bibr" rid="bibr22-0098628311430642">Bollag, 2006</xref>). As with many PAs, this assignment was assessed with a rubric, which is a set of criteria or standards used in the qualitative evaluation of a performance.</p>
<p>Sometimes, students can be authentically assessed as they do actual work in psychology, such as professional work at an internship site; but often authentic assessments involve some degree of simulation of actual psychological tasks. Simulation can be a good tool to increase authenticity because it allows students to work on tasks, answer questions, solve problems, and make decisions when working on real-world tasks is not practically feasible. For example, casebooks can be used to teach students CT skills for analyzing cases simulating the presentation of actual symptoms.</p>
<p>Recently, Barron and students at JMU used simulation in a new CT research analysis assessment task in which students read a fictitious research article and then critiqued it (K. Barron, A. Swinson, &amp; A. Marston, personal communication, July 31, 2010). Students were assessed using both quantitative and qualitative measures on a number of variables including their ability to identify the main points and the strengths and weaknesses of the article. Related to CT assessment, students identified the design of the study, evaluated the logic and clarity of statistical presentation, and evaluated the presence of researcher bias.</p>
<p>The FSU Psychology Department has used simulation extensively in exercises and assessments that include questions students may encounter in psychology and everyday life as a way to teach and assess argument analysis skills (e.g., <xref ref-type="bibr" rid="bibr14-0098628311430642">Bensley, Crow et al., 2010</xref>). Although performance on authentic tasks is often assessed subjectively with rubrics, Bensley et al. used a homegrown multiple-choice test to objectively assess students’ ability to analyze and evaluate arguments in the context of psychological practice and everyday situations. Test questions were based on methods for analyzing arguments and evaluating the quality of evidence found in <xref ref-type="bibr" rid="bibr9-0098628311430642">Bensley (1998)</xref>.</p>
<p>Another advantage of simulation is the high degree of manipulation and control it can provide in LOA studies. FSU has found simulation to be a powerful tool in the construction of instructional and assessment materials, enabling systematic analysis of CT learning outcomes. For example, to test students’ ability to critically evaluate information on the Internet, FSU used simulation to develop mock websites resembling actual websites. While controlling readability and other textual characteristics of the sites, one mock website was supported by high-quality research evidence, such as experiments and supportive literature reviews, whereas the other was supported by low-quality evidence, such as testimonials. <xref ref-type="bibr" rid="bibr16-0098628311430642">Bensley, Powell, and Flinn (2010)</xref> asked research methods students to read and rate the two mock websites in counterbalanced order. Students reliably rated the high-quality mock website as significantly more credible, accurate, reasonable, and better supported by higher quality evidence than the lower quality one. These results should encourage the use of homegrown measures when they are carefully constructed; but often homegrown measures lack information about their reliability and validity, raising issues about their quality.</p>
</sec>
<sec id="section6-0098628311430642">
<title>5. Use the Best and Most Appropriate Measures Available</title>
<p>Deciding what are the best and most appropriate measures for testing an assessment hypothesis involves considering the content and form of instruction, the quality of measures, and the relevant CT dimension(s). Because CT is a multidimensional construct and measurement instruments are limited in what they can assess, it follows that multiple measures of CT should be used to ensure effective assessment of CT and convergent validity. With this in mind, the review of assessment instruments and measures that follows highlights their strengths and weaknesses.</p>
<p>Those most concerned with the quality of LOA data often select standardized, objective tests with good reliability, validity, and group norms. Two of the most commonly used standardized CT tests, the Watson-Glaser Critical Thinking Appraisal and the Cornell Critical Thinking Test, have good reliability and validity (<xref ref-type="bibr" rid="bibr31-0098628311430642">Follman, 2003</xref>; <xref ref-type="bibr" rid="bibr33-0098628311430642">Gadzella et al., 2006</xref>). Although tests like these purport to measure specific components of argument analysis skill, they are probably better measures of general CT skill (<xref ref-type="bibr" rid="bibr18-0098628311430642">Bernard et al., 2008</xref>). After instruction, psychology students have shown increases on the Cornell Critical Thinking Test, Level Z (e.g., <xref ref-type="bibr" rid="bibr55-0098628311430642">Nieto &amp; Saiz, 2008</xref>; <xref ref-type="bibr" rid="bibr65-0098628311430642">Solon, 2007</xref>) and on the Watson-Glaser test (<xref ref-type="bibr" rid="bibr52-0098628311430642">McLean &amp; Miller, 2010</xref>). The JMU Psychology Department has successfully used the Cornell test to assess its students but has moved to a CT test developed by the JMU Philosophy Department (<xref ref-type="bibr" rid="bibr8-0098628311430642">Apple, Serdikoff, Reis-Bergan, &amp; Barron, 2008</xref>). For more information on standardized CT ability tests, see <xref ref-type="bibr" rid="bibr31-0098628311430642">Follman (2003)</xref>, <xref ref-type="bibr" rid="bibr36-0098628311430642">Halonen (2008)</xref>, or <xref ref-type="bibr" rid="bibr61-0098628311430642">Possin (2008)</xref>.</p>
<p>Often, no standardized tests or measures are available to assess a specific type of CT, and the best assessment researchers can do is to develop their own instruments. <xref ref-type="bibr" rid="bibr44-0098628311430642">Lawson (1999)</xref> and his colleagues developed the Mount St. Joseph psychological critical thinking test to assess psychological CT because no test was available. Supporting its content validity, it was based on principles for analyzing psychological research proposed by three different authors of CT books. Supporting its predictive validity, senior psychology majors scored better on the test than introductory psychology students (<xref ref-type="bibr" rid="bibr44-0098628311430642">Lawson, 1999</xref>), and students receiving relevant CT instruction scored better on it than those not receiving instruction (<xref ref-type="bibr" rid="bibr59-0098628311430642">Penningroth et al., 2007</xref>; <xref ref-type="bibr" rid="bibr76-0098628311430642">Williams, Oliver, Allin, Winn, &amp; Booher, 2003</xref>); however, more research is needed especially on its reliability.</p>
<p>Self-report data can provide useful information about attitudes, dispositions, experiences, and metacognitions related to CT that are not captured by ability measures. For example, the JMU Psychology Department collected self-report ratings on how well students thought they had achieved the goal of acquiring CT skills and how well the program provided them opportunities to achieve it, followed by open-ended explanations for their ratings (<xref ref-type="bibr" rid="bibr8-0098628311430642">Apple et al., 2008</xref>). JMU has also used an inventory developed by <xref ref-type="bibr" rid="bibr43-0098628311430642">Kruger and Zechmeister (2001)</xref> asking students to report their experiences with CT and other skills in the program.</p>
<p>In contrast, FSU has focused more on the use of self-report instruments for assessing CT dispositions such as the California Critical Thinking Dispositions Inventory of <xref ref-type="bibr" rid="bibr30-0098628311430642">Facione and Facione (1992)</xref>, a widely used self-report measure of general CT dispositions with good psychometric properties (<xref ref-type="bibr" rid="bibr31-0098628311430642">Follman, 2003</xref>). FSU has used other self-report instruments of CT dispositions developed mainly for research purposes, such as a short form of the Need for Cognition (NFC) scale of <xref ref-type="bibr" rid="bibr24-0098628311430642">Cacioppo, Petty, and Kao (1984)</xref> adapted from <xref ref-type="bibr" rid="bibr56-0098628311430642">Pacini and Epstein (1999)</xref>. The NFC is a reliable measure of intellectual engagement that has shown concurrent validity with measures of reasoning and openness to new ideas (<xref ref-type="bibr" rid="bibr23-0098628311430642">Cacioppo, Petty, Feinstein, &amp; Jarvis, 1996</xref>). FSU has also used the Objectivism Scale of <xref ref-type="bibr" rid="bibr45-0098628311430642">Leary, Shepperd, McNeill, Jenkins, and Barnes (1986)</xref>, a reliable and valid instrument measuring the inclination to value objective evidence; the Flexible Thinking Scale of <xref ref-type="bibr" rid="bibr67-0098628311430642">Stanovich and West (1997)</xref>; and the Openness to Ideas subscale of the NEO Personality Inventory (<xref ref-type="bibr" rid="bibr28-0098628311430642">Costa &amp; McCrae, 1992</xref>).</p>
<p>To measure CT dispositions in psychology, the FSU Psychology Department developed its own self-report instrument called the Inventory of Thinking Dispositions in Psychology (ITDP), designed to measure active, open-minded interest in psychology and the disposition to take a reasonable, scientific approach to psychology (<xref ref-type="bibr" rid="bibr13-0098628311430642">Bensley &amp; Bates, 2006</xref>). Recent research by <xref ref-type="bibr" rid="bibr57-0098628311430642">Parsons, Powell, Bensley, and Crowe (2010)</xref> found that the ITDP had good test–retest reliability (<italic>r</italic> = .72, <italic>p</italic> &lt; .001) in beginning majors tested a second time after 4 weeks and good internal consistency, (α = .87). Supporting its concurrent validity, they found a significant correlation with the NFC (<italic>r</italic> = .57, <italic>p</italic> &lt; .01). Results of assessments with the ITDP, NFC, and other CT dispositions measures have been mixed, as described later.</p>
<p>A limitation of both objective tests and self-report measures of CT is their inability to indicate much about the language and thoughts generated by students as they reason or solve a problem. In such cases, rubrics and other qualitative measures may provide additional information. Despite the lack of research on rubrics, it is often claimed that they help guide and improve student thinking. In coming to understand scoring and feedback with rubrics, both faculty and students are said to develop clearer conceptions of how certain kinds of work meet criteria for good thinking and others do not (<xref ref-type="bibr" rid="bibr5-0098628311430642">Andrade, 2000</xref>). It has also been claimed that when instructors inform students of scoring criteria beforehand, this can help students guide their work to increase its quality (<xref ref-type="bibr" rid="bibr36-0098628311430642">Halonen, 2008</xref>), perhaps promoting self-regulation. These claims about the benefits of rubrics and the considerable attention paid to them in the literature (e.g., <xref ref-type="bibr" rid="bibr37-0098628311430642">Halonen, Bosack, Clay, &amp; McCarthy, 2003</xref>; <xref ref-type="bibr" rid="bibr73-0098628311430642">Thaler, Kazemi, &amp; Huscher, 2009</xref>) suggest that rubrics may be a practical solution to the problem of how to teach and assess CT authentically; but rubrics have potential problems that should be considered before using them.</p>
<p>An ongoing concern is the insufficient attention paid to the reliability of performance assessments and rubrics. One of the historical reasons for the replacement of traditional performance assessments with multiple-choice tests in the early 20th century was the difficulty in obtaining accurate and reliable scores on performance assessments (<xref ref-type="bibr" rid="bibr26-0098628311430642">Clauser, 2000</xref>). Since then, little research has been done on the reliability of rubrics used in psychology to assess learning outcomes. When this research has been conducted, it has sometimes proven difficult to develop reliable rubrics despite considerable effort devoted to developing objective criteria (<xref ref-type="bibr" rid="bibr68-0098628311430642">Stellmack, Konheim-Kalkstein, Manor, Massey, &amp; Schmitz, 2009</xref>). Measures of CT, whether subjective or objective, must be reliable and valid to be scientifically useful (<xref ref-type="bibr" rid="bibr75-0098628311430642">Williams, 1999</xref>); and departments should attend to these issues as they use rubrics. Although AC has done little research on the reliability of their rubrics, they have used qualitative ratings by faculty of follow-up interviews with graduates in work and everyday settings to validate their approach (<xref ref-type="bibr" rid="bibr54-0098628311430642">Mentkowski &amp; Associates, 2000</xref>; <xref ref-type="bibr" rid="bibr63-0098628311430642">Rogers &amp; Mentkowski, 2004</xref>).</p>
<p>Another concern for rubric users is that their intuitions about the meaning of natural language terms may seduce them into underestimating the difficulty of selecting terms to represent rubric dimensions that validly measure complex constructs like CT. One way this can happen is when rubric developers choose language descriptors for a single criterion that do not all refer to the same dimension. This is especially a problem if numeric scores assigned to the criteria along a dimension are assumed to be at least ordinal level. For example, <xref ref-type="bibr" rid="bibr5-0098628311430642">Andrade (2000)</xref> developed an instructional rubric to score persuasive essays with language that seemed to shift dimensions along a single criterion. Specifically, Andrade recommended that the criterion “The claim” be assigned a 2 for “My claim is buried, confused, and/or unclear” and a 1 for “I don’t say what my argument or claim is” (p. 17). Including “argument” with “claim” changes the dimension being evaluated because technically arguments are claims or conclusions supported by evidence. This is an important distinction that often escapes many students. Problems with the meaning of dimensions used in rubrics as well as their frequent lack of reliability and validity information urge caution in the use of rubrics, especially as summative assessments.</p>
</sec>
<sec id="section7-0098628311430642">
<title>6. Conduct Assessments That Are Sensitive to Changes Over Time</title>
<p>Because learning is commonly defined as a process in which changes in knowledge occur because of experience or practice over time, it is difficult to argue that student learning outcomes can be meaningfully evaluated without taking time into account. <xref ref-type="bibr" rid="bibr38-0098628311430642">Halpern (1988)</xref> has urged programs to take a “value added” approach to help determine what a program has contributed to the change in students over time. Simply testing seniors once in their capstone courses is not sufficient to infer changes over time because the levels of skill and knowledge of students entering the program are unknown.</p>
<p>In terms of research methodology, assessment studies are typically designed in one of five basic ways to assess the value-added component: (a) simple pretest–posttest, (b) pretest–posttest with control group, (c) cross-sectional, (d) longitudinal, and (e) long-sectional (a mixed design combining longitudinal and cross-sectional designs). <xref ref-type="table" rid="table1-0098628311430642">Table 1</xref>
 describes these designs, their typical uses in LOA research, and summarizes their strengths and weaknesses.</p>
<table-wrap id="table1-0098628311430642" position="float">
<label>Table 1.</label>
<caption>
<p>Designs for Assessing the Value Added by Instruction and Programs</p>
</caption>
<graphic alternate-form-of="table1-0098628311430642" xlink:href="10.1177_0098628311430642-table1.tif"/>
<table>
<thead>
<tr>
<th>Design</th>
<th>Level</th>
<th>Description</th>
<th>Strengths</th>
<th>Weaknesses</th>
</tr>
</thead>
<tbody>
<tr>
<td>Simple pretest–posttest (repeated measures)</td>
<td>Specific teaching technique, often in classroom</td>
<td>Measure change in a single group before and after instruction</td>
<td>Relatively quick initial assessment of a specific instructional technique, typically 1 semester or less</td>
<td>Unable to rule out alternative hypotheses as to the cause of change<break/>Cannot assess overall program</td>
</tr>
<tr>
<td>Pretest–posttest with control group, usually intact groups</td>
<td>Specific teaching technique, often in classroom</td>
<td>Measure change before and after instruction between groups receiving different instruction</td>
<td>Other sections of the course can serve as control groups<break/>Reduces likelihood of alternative hypotheses for cause of change<break/>Relatively quick to complete, typically 1 semester or less<break/>Within-subjects design results in a smaller error term, decreasing risk of Type II error</td>
<td>Does not determine causation unless true experimental conditions are met (random assignment of students to classes/groups)<break/>Internal validity is threatened to the extent that comparison groups are dissimilar (i.e., different instructors or courses, etc.)<break/>Cannot assess overall program</td>
</tr>
<tr>
<td>Cross-sectional</td>
<td>Overall program</td>
<td>Assess groups of students at different levels of the program, tested at the same time, such as beginning versus finishing majors</td>
<td>Provides immediate initial comparisons between students at different levels of the program</td>
<td>Differences may be because of cohort effects rather than program effects<break/>Between-subjects design results in larger error term, increasing risk of Type II error </td>
</tr>
<tr>
<td>Longitudinal</td>
<td>Overall program</td>
<td>Assess the same cohort during the course of the program, often testing majors when beginning and finishing the program</td>
<td>Controls for cohort effects<break/>Within-subjects design results in a smaller error term, decreasing risk of Type II error</td>
<td>Takes years to complete<break/>Difficult to keep individual student’s data linked<break/>Students may drop out</td>
</tr>
<tr>
<td>Long-sectional, or a mixed design combining longitudinal and cross-sectional</td>
<td>Overall program</td>
<td>Assess students at different levels of the program over time, typically during the course of the full program (or between programs)</td>
<td>Provides immediate initial comparisons between students at different levels of the program<break/>Over time, it controls for cohort effects<break/>Can compare different programs and majors<break/>Within-subjects design results in a smaller error term, decreasing risk of Type II error</td>
<td>Takes years to complete the full assessment<break/>Difficult to keep individual student’s data linked<break/>Students may drop out</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>All the designs in <xref ref-type="table" rid="table1-0098628311430642">Table 1</xref> are quasi-experimental, except when the pretest–posttest design with a control group has random assignment. True experiments that allow for causal inferences can sometimes be conducted (<xref ref-type="bibr" rid="bibr49-0098628311430642">LoSchiavo, Shatz, &amp; Poling, 2008</xref>); however, they often are not feasible because of curricular constraints and the difficulty of randomly assigning students to control groups (e.g., <xref ref-type="bibr" rid="bibr55-0098628311430642">Nieto &amp; Saiz, 2008</xref>). Even when random assignment is not possible, the pretest–posttest with control group design is a powerful tool for studying different forms of instruction when differences in treatment classes and comparison groups are minimized.</p>
</sec>
<sec id="section8-0098628311430642">
<title>7. Assess Frequently, Embedding Assessment and Feedback Into Instruction</title>
<p>Theoretically, assessing more often could provide more reliable information about when and under what conditions learning occurs. Yet, assessing students too much, especially with the same instruments, can sensitize them to measures—producing method variance, practice effects, and decreased effort. Frequent assessments with the same instrument are especially problematic if the assessments are serving as summative assessment.</p>
<p>In contrast, many educators who take a formative assessment approach embed frequent assessments within instruction to produce learning effects from practice. One of the most highly developed of these approaches is AC’s “assessment as learning approach” (<xref ref-type="bibr" rid="bibr3-0098628311430642">Alverno College Faculty, 1994</xref>; <xref ref-type="bibr" rid="bibr34-0098628311430642">Graham, 1998</xref>). The assumption is that assessment is not just for evaluation of students but also to help them focus on important skills and feedback to improve their learning. Considerable research supports the positive effects of feedback in formative assessments (<xref ref-type="bibr" rid="bibr21-0098628311430642">Black &amp; Wiliam, 1998</xref>). Extending the idea of embedded assessment, <xref ref-type="bibr" rid="bibr51-0098628311430642">McCarthy, Niederjohn, and Bosack (2011)</xref> have argued that regular assessments embedded within courses can serve as both formative assessments and ways of evaluating instructors and courses.</p>
<p>Of the three programs featured, AC has used individual feedback the most systematically and thoroughly over time. Students are given feedback, usually by way of rubrics and explicit criteria within classes, but also programmatically. Recently, the AC Psychology Department has adapted the college’s “academic concerns form” to track academic problems that individual students may show from class to class. These are reported to students so that they will address these concerns and be able to better progress through the program (K. Vasquez, personal communication, May 27, 2010). The strategic use of feedback could help students realize the need for greater self-regulation of their own thinking often associated with CT (<xref ref-type="bibr" rid="bibr39-0098628311430642">Halpern, 1998</xref>).</p>
<p>The FSU Psychology Department has taken a different formative assessment approach, using more objective measures in both learning and instruction. FSU has used a form of explicit, guided instruction called “direct infusion,” in which students are explicitly taught rules and criteria for CT in the context of thinking about their psychology course work. Students complete exercises requiring them to apply these rules, criteria, and relevant knowledge to think about questions and problems in psychology. Then, they receive feedback about their performance on these exercises and take quizzes with similar questions that differ in content and receive additional feedback (<xref ref-type="bibr" rid="bibr11-0098628311430642">Bensley, 2010</xref>). To assess potential gains on summative assessments, students are typically pretested at the beginning of the course and posttested at its conclusion on a test assessing the same skills infused in instruction but differing in content. The emphasis in direct infusion on targeting thinking skills to be explicitly taught in the classroom and use of formative assessments with feedback share key components with other approaches to teaching CT (e.g., <xref ref-type="bibr" rid="bibr6-0098628311430642">Angelo, 1995</xref>; <xref ref-type="bibr" rid="bibr20-0098628311430642">Beyer, 1997</xref>). Formative assessments can be useful in helping to guide and improve CT, especially in the classroom; but the direct infusion approach, unlike the embedded assessment approach of <xref ref-type="bibr" rid="bibr51-0098628311430642">McCarthy et al. (2011)</xref>, also includes summative assessments of CT that are both aligned with formative assessments and contribute to both course and program evaluation. When appropriately interpreted, these summative assessments can be used to strategically evaluate progress after specific courses and provide process information about attainments of CT outcomes at various points in the program.</p>
</sec>
<sec id="section9-0098628311430642">
<title>8. Interpret Assessment Results Cautiously and Apply the Results Appropriately</title>
<p>Collecting high-quality assessment data should help determine what is working and not working; but as with other research, caution should be exercised in interpreting results. Making a decision to change a program based on assessment data without considering the quality of the data can have far reaching negative consequences. Accordingly, to make appropriate interpretations of assessment data, LOA researchers should consider the representativeness and size of samples, reliability and validity of measures, and the purpose for collecting the data. The more students sampled, the better inferences can be made. The three programs have handled the sampling problem in different ways. In their study of critical reading and CT dispositions, FSU assessed all seniors and beginning students in the 2009–2010 academic year. In other assessment studies, they have compared students in different sections of the same course or similar courses at different points in the curriculum, often aggregating the data from similar classes to increase statistical power. AC, with fewer majors, obtains representative samples by repeatedly assessing all of their students in every class and at the end of their program. In contrast, because JMU has so many majors and different assessments, they often randomly select subsets of their seniors to complete different assessments (K. Apple, K. Barron, &amp; T. Zinn, personal communication, July 31, 2010).</p>
<p>The less that is known about measures, the more caution is warranted. Standardized tests and research instruments often have known psychometric properties, and data should be available for comparison with a normative sample. However, when programs like AC, FSU, and JMU develop their own tests, they have more difficulty comparing their results with those of students outside of their own institution. Both JMU and FSU have begun studying the reliability of their homegrown instruments, but more research is needed. Likewise, AC and those who use rubrics should study their reliability. Despite claims made about the validity of performance assessments, they cannot ultimately be valid if the rubrics used to assess performance are not, first, reliable.</p>
<p>Even when reliable and valid measurements have been made on representative samples, it is difficult to draw good conclusions about abilities when motivation for participating in assessment activities was low or unknown. CT assessment may be especially dependent on proper motivation because CT is an effortful activity (<xref ref-type="bibr" rid="bibr39-0098628311430642">Halpern, 1998</xref>). AC, FSU, and JMU have taken different approaches to assessment motivation. To control for motivation related to instructor, AC employs external assessments in which faculty who are not the instructors of classes in which assessments occur conduct the assessments. This communicates the importance of assessment, and they have evidence that students take the assessments more seriously (K. Vasquez, personal communication, May 27, 2010).</p>
<p>To foster a culture of assessment, FSU publishes information about required assessments in the syllabi of beginning and senior capstone courses where assessment is expected. Students in the beginning and senior courses receive course credit for their participation. They are informed of the importance of the assessment activities for evaluating and improving instruction and are urged to follow directions carefully and to do their best. In some FSU classes, where students have rated the effort they expended in completing critical reading tests, they have on average reported a moderate amount of effort applied.</p>
<p>JMU has perhaps done the most research on student motivation for assessment, administering a 10-item, self-report instrument called the Student Opinion Scale (SOS) to more than 15,000 of its students participating on their annual assessment day, when all JMU students are expected to complete various assessments. <xref ref-type="bibr" rid="bibr70-0098628311430642">Sundre and Moore (2002)</xref> modified the SOS from an instrument originally developed by <xref ref-type="bibr" rid="bibr77-0098628311430642">Wolf and Smith (1995)</xref> so that it measured effort and importance. Reliabilities for the SOS and its subscales have ranged from .80 to .89 in low-stakes assessments, and it has shown good concurrent validity with other measures of academic motivation. <xref ref-type="bibr" rid="bibr70-0098628311430642">Sundre and Moore (2002)</xref> also found that the vast majority of JMU students reported making a good effort in assessment activities but students were less motivated on arduous tasks. This could have implications for performance on CT tasks, which are often effortful. Recently, the JMU Psychology Department found that SOS ratings of their students for effort ranged from 3.84 to 3.96 on a 5-point scale, depending on the type of assessments conducted (K. Barron, personal communication, July 31, 2010).</p>
<p>To guide interpretation of data from multiple measures, it helps to organize the findings in relation to the goals and objectives of the assessment. <xref ref-type="bibr" rid="bibr69-0098628311430642">Stoloff et al. (2004)</xref> have provided many useful recommendations and a table of how JMU has used a variety of assessment study results to close the loop (using assessment finding to make changes in the program). To further illustrate this process, <xref ref-type="table" rid="table2-0098628311430642">Table 2</xref>
 describes four recent CT outcome assessment studies at FSU and how the results were interpreted and applied. <xref ref-type="table" rid="table2-0098628311430642">Table 2</xref> also shows how FSU simultaneously tested the direction infusion hypothesis while assessing its courses and program.</p>
<table-wrap id="table2-0098628311430642" position="float">
<label>Table 2.</label>
<caption>
<p>Interpretation and Application of Critical Thinking Assessment Research Findings of the Frostburg State University Psychology Department</p>
</caption>
<graphic alternate-form-of="table2-0098628311430642" xlink:href="10.1177_0098628311430642-table2.tif"/>
<table>
<thead>
<tr>
<th>Assessment Hypotheses</th>
<th>Method</th>
<th>Findings</th>
<th>Interpretation</th>
<th>Response to Results</th>
</tr>
</thead>
<tbody>
<tr>
<td>Compared with beginning majors, senior majors should<list list-type="order">
<list-item>
<p>have better critical reading skill</p>
</list-item>
<list-item>
<p>be more disposed to think critically</p>
</list-item>
<list-item>
<p>be more likely to revise their beliefs to make them consistent with evidence</p>
</list-item>
</list>
</td>
<td>Compared 53 seniors in capstone courses with 90 beginning majors on<list list-type="order">
<list-item>
<p>CRT titled “How Much of the Brain Do You Use?”</p>
</list-item>
<list-item>
<p>Need for Cognition, NEO-Openness, Flexible Thinking scale</p>
</list-item>
<list-item>
<p>rate belief in 10% myth before and after taking CRT</p>
</list-item>
</list>
</td>
<td>
<list list-type="order">
<list-item>
<p>Seniors did not do significantly better than beginning majors on CRT.</p>
</list-item>
<list-item>
<p>Seniors had significantly higher scores on a measure of CT dispositions.</p>
</list-item>
<list-item>
<p>CT skill and disposition were both negatively correlated with belief.</p>
</list-item>
</list>
</td>
<td>
<list list-type="order">
<list-item>
<p>The program did not seem to have produced better critical reading skills.</p>
</list-item>
<list-item>
<p>It may have increased CT disposition in seniors who scored higher than beginning majors.</p>
</list-item>
<list-item>
<p>Students with greater CT skill and disposition tended to revise their belief more.</p>
</list-item>
</list>
</td>
<td>Further investigate the conditions that improve CT. It may be that more explicit critical reading instruction is needed as well as more emphasis on fostering CT dispositions and belief revision when appropriate.</td>
</tr>
<tr>
<td>Students in RM courses should improve their argument analysis skills, but we expected they would improve only after direct infusion of argument analysis skills.</td>
<td>Compared students from a beginning RM course (<italic>N</italic> = 12) receiving direct infusion of CT skills with a similar beginning RM course (<italic>N</italic> = 19) and a more advanced RM course (<italic>N</italic> = 16), both receiving traditional instruction; tested at the beginning and end of the course on APS.</td>
<td>Only the RM course getting direct infusion of CT skills showed significant gains in argument analysis and showed significantly greater gains than the students in the traditionally taught RM control group courses.</td>
<td>Direct infusion of argument analysis skill was more effective than traditional RM instruction in improving argument analysis skill in RM students.</td>
<td>We should attempt to replicate the results with larger samples and in other courses. If findings hold, explore revising RM course to include direct infusion of argument analysis skills for evaluating information and writing research reports.</td>
</tr>
<tr>
<td>We expected that a class receiving direct infusion of argument analysis and critical reading skills would show significantly greater gains than a similar control class on<list list-type="order">
<list-item>
<p>an argument analysis test and</p>
</list-item>
<list-item>
<p>a critical reading test.</p>
</list-item>
</list>
</td>
<td>One cognitive psychology class received direct infusion of argument analysis and critical reading skills (N = 18) whereas the other control class (N = 23) did not. We compared the two cognitive psychology classes on<list list-type="order">
<list-item>
<p>APS</p>
</list-item>
<list-item>
<p>CRT titled Memory and Aging</p>
</list-item>
</list>
</td>
<td>The CT direct infusion class showed significantly greater gains than the control class on<list list-type="order">
<list-item>
<p>argument analysis and</p>
</list-item>
<list-item>
<p>critical reading.</p>
</list-item>
</list>
</td>
<td>Direct infusion of argument analysis and critical reading skills was more effective than traditional instruction in increasing CT skills for argument analysis and critical reading.</td>
<td>We should directly infuse argument analysis and critical reading skills into future classes and attempt to replicate the results with more students and alternate forms of the tests. </td>
</tr>
<tr>
<td>We expected that RM students given direct infusion of CT skills for methodological and causal reasoning would gain significantly on a test of methodological and causal reasoning skills than RM students traditionally taught.</td>
<td>We compared students from RM courses directly infused with methodological and causal reasoning skills (<italic>N</italic> = 35) with traditionally taught RM classes not receiving explicit CT instruction (<italic>N</italic> = 41), before and after instruction, on the Thinking Critically About Research Test.</td>
<td>Students from the CT-infused courses showed significantly greater gains than traditionally taught students on the test of methodological and causal reasoning test.</td>
<td>Direct infusion of methodological and causal reasoning skills was more effective in teaching these skills than traditional instruction in the RM classes compared.</td>
<td>Explore revising the new RM course to include direct infusion of methodological and causal reasoning skills into all sections, possibly putting instructional and assessment materials online.</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0098628311430642">
<p>APS = Analyzing Psychological Statements; CRT = Critical Reading Test; CT = critical thinking; RM = Research Methods.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>The results summarized in <xref ref-type="table" rid="table2-0098628311430642">Table 2</xref> show that the direct infusion approach for explicitly teaching CT was effective in producing improvements in various CT skills in different upper level courses. These results are consistent with numerous other studies that have found explicit infusion of CT skills was effective in various disciplines (<xref ref-type="bibr" rid="bibr1-0098628311430642">Abrami et al., 2008</xref>; <xref ref-type="bibr" rid="bibr25-0098628311430642">Chance, 1986</xref>) and in psychology (e.g., <xref ref-type="bibr" rid="bibr15-0098628311430642">Bensley &amp; Haynes, 1995</xref>; <xref ref-type="bibr" rid="bibr55-0098628311430642">Nieto &amp; Saiz, 2008</xref>; <xref ref-type="bibr" rid="bibr59-0098628311430642">Penningroth et al., 2007</xref>; <xref ref-type="bibr" rid="bibr65-0098628311430642">Solon, 2007</xref>). Likewise, this interpretation may also explain the failure of FSU seniors to show significantly better critical reading performance than beginning students (<xref ref-type="table" rid="table2-0098628311430642">Table 2</xref>, study 1), as at least partly because of the lack of explicit CT skill instruction in many FSU courses.</p>
<p>This pattern of findings seems to have implications for improvement of the FSU Psychology program and future assessment (Frostburg State University Psychology Department, 2010). One is that direct infusion of CT skills should be implemented more systematically throughout the curriculum. Recently, the FSU Psychology Department has revised its curriculum to include a critical thinking and scientific inquiry course to be taken immediately after General Psychology, as well as a revised introductory research methods course to follow it. Because the required CT course uses the direct infusion approach and is placed at the beginning of the curriculum, direct infusion of CT skills can be delivered to all majors and coordinated with the direct infusion of methodological and causal reasoning skills in the new research methodology course. Making these changes should improve seniors’ critical reading and other CT skills, resulting in seniors performing significantly better on CT tests than beginning majors in future LOA studies.</p>
<p>One final cautionary note is in order concerning interpretation of outcome assessment results. Sometimes, those discussing assessment results use interpretive language that goes beyond the data. They may speak of the improvement of CT or some other construct related to learning as if the changes observed in measures reflected actual changes in underlying cognitive processes, or that from assessment results they may be able to diagnose a student’s actual cognitive shortcomings. LOA data typically allows for measurement of student performance, in general, and not diagnosis of changes within the underlying cognitive systems. Very few, if any, commonly used LOA measures are designed to do cognitive diagnosis, although developing such measures is a worthy objective for future LOA research, especially research using qualitative measures (<xref ref-type="bibr" rid="bibr47-0098628311430642">Leighton &amp; Gierl, 2007</xref>).</p>
</sec>
</sec>
<sec id="section10-0098628311430642">
<title>Conclusion</title>
<p>In this article, we made several suggestions for how to take a scientific approach to assessing CT. Because CT is a complex construct including skills, dispositions, and metacognition, this presents several challenges to those seeking to comprehensively assess CT. To capture the multidimensional nature of CT, all three of the featured programs have recognized the importance of using multiple measures to assess CT. In the future, however, LOA researchers should seek to capture the dispositions and metacognitions of students as they perform thinking tasks and not just follow the common practice of assessing their CT skills.</p>
<p>Programs can also increase the validity of their approaches by using both qualitative and objective measures to better understand student learning and thinking. All three programs have developed their own homegrown measures, often using simulation to authentically assess outcomes related to “thinking like a psychologist.” Although this approach can be very useful and increase the ecological validity of CT assessment, LOA researchers developing new measures should demonstrate that they are both reliable and valid (<xref ref-type="bibr" rid="bibr75-0098628311430642">Williams, 1999</xref>).</p>
<p>Implementing all the suggestions proposed in this article may seem daunting, especially to faculty in small departments or those receiving little institutional support. Nevertheless, we are confident that taking a scientific approach that begins by focusing on a more limited number of specific CT outcomes followed by strategic administration of appropriate measures that allow for assessing the value added can improve the quality of assessment data. Furthermore, when outcomes are systematically studied in relation to instruction and faculty are willing to change instruction in response to high-quality data, measureable improvement is likely to be observed. In the process, we hope that psychologists conducting LOA studies will realize that they are scientists studying important questions in applied psychological science that can expand their own knowledge and the knowledge of the field.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We thank Selena Smith of the Frostburg State University Office of Institutional Research for helping us obtain academic background information on participants and the student participants who consented to let us use their data. Additionally, we thank our assessment graduate assistants: Allison Bates, Vanessa Rowan, Deborah Crowe, Lauren Powell, and Rachel Spero for helping us with various assessment research projects described in this article. We also thank Kris Vasquez of Alverno College and Kevin Apple, Kenn Barron, and Tracy Zinn from James Madison University for updating us on the recent developments in assessment of their programs. Thanks to Patricia Marsh from the University of Central Missouri for helpful comments on an earlier draft of the article.</p>
<p>The first author thanks Frostburg State University for granting him a sabbatical leave that made it possible to write articles and to complete the reporting of our assessment research findings; he also thanks the Frostburg State University Foundation for funding both students and faculty to present assessment research at conferences. He also wishes to remember and acknowledge the many contributions of his colleague and friend, Chrismarie Baxter, who profoundly influenced Frostburg State University’s assessment efforts.</p>
<p>Thanks also to Andrew Christopher and three anonymous reviewers who provided insightful comments that helped us revise the manuscript of this article.</p>
</ack>
<fn-group>
<fn fn-type="conflict" id="fn1-0098628311430642">
<p>The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure" id="fn2-0098628311430642">
<p>The authors received no financial support for the research, authorship, and/or publication of this article.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Abrami</surname>
<given-names>P. C.</given-names>
</name>
<name>
<surname>Bernard</surname>
<given-names>R. M.</given-names>
</name>
<name>
<surname>Borokhovski</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Wade</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Surkes</surname>
<given-names>M. A.</given-names>
</name>
<name>
<surname>Tamim</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Zhang</surname>
<given-names>D</given-names>
</name>
</person-group>. (<year>2008</year>). <article-title>Instructional interventions affecting critical thinking skills and dispositions: A stage 1 meta-analysis</article-title>. <source>Review of Educational Research</source>, <volume>4</volume>, <fpage>1102</fpage>–<lpage>1134</lpage>. <comment>doi:10.3102/0034654308326084</comment>
</citation>
</ref>
<ref id="bibr2-0098628311430642">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Allen</surname>
<given-names>M. J.</given-names>
</name>
</person-group> (<year>2004</year>). <source>Assessing academic programs in higher education</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Anker</publisher-name>.</citation>
</ref>
<ref id="bibr3-0098628311430642">
<citation citation-type="book">
<collab collab-type="author">Alverno College Faculty</collab>. (<year>1994</year>). <source>Student assessment-as-learning at Alverno College</source>. <publisher-loc>Milwaukee, WI</publisher-loc>: <publisher-name>Alverno Institute</publisher-name>.</citation>
</ref>
<ref id="bibr4-0098628311430642">
<citation citation-type="web">
<collab collab-type="author">American Psychological Association Task Force on Undergraduate Major Competencies</collab>. (<year>2002</year>). <source>Undergraduate psychology major learning goals and outcomes: A report</source>. <comment>Retrieved from</comment> <ext-link ext-link-type="uri" xlink:href="www.apa.org/ed/resources.html">www.apa.org/ed/resources.html</ext-link>
</citation>
</ref>
<ref id="bibr5-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Andrade</surname>
<given-names>H. G.</given-names>
</name>
</person-group> (<year>2000, February</year>). <article-title>Using rubrics to promote thinking and learning</article-title>. <source>Educational Leadership</source>, <fpage>13</fpage>–<lpage>18</lpage>.</citation>
</ref>
<ref id="bibr6-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Angelo</surname>
<given-names>T. A</given-names>
</name>
</person-group>. (<year>1995</year>). <article-title>Classroom assessment for critical thinking</article-title>. <source>Teaching of Psychology</source>, <volume>22</volume>, <fpage>6</fpage>–<lpage>7</lpage>. <comment>doi:10.1207/s15328023top2201_1</comment>
</citation>
</ref>
<ref id="bibr7-0098628311430642">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Angelo</surname>
<given-names>T. A.</given-names>
</name>
<name>
<surname>Cross</surname>
<given-names>K. P.</given-names>
</name>
</person-group> (<year>1993</year>). <source>Classroom assessment techniques: A handbook for college teachers</source> (<edition>2nd ed.</edition>). <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation>
</ref>
<ref id="bibr8-0098628311430642">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Apple</surname>
<given-names>K. J.</given-names>
</name>
<name>
<surname>Serdikoff</surname>
<given-names>S. L.</given-names>
</name>
<name>
<surname>Reis-Bergan</surname>
<given-names>M. J.</given-names>
</name>
<name>
<surname>Barron</surname>
<given-names>K. E.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Programmatic assessment of critical thinking</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Dunn</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Halonen</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Smith</surname>
<given-names>R.</given-names>
</name>
</person-group> (Eds.), <source>Teaching critical thinking in psychology: A handbook of best practices</source> (pp. <fpage>77</fpage>–<lpage>88</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Wiley-Blackwell</publisher-name>.</citation>
</ref>
<ref id="bibr9-0098628311430642">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Bensley</surname>
<given-names>D. A.</given-names>
</name>
</person-group> (<year>1998</year>). <source>Critical thinking in psychology: A unified skills approach</source>. <publisher-loc>Pacific Grove, CA</publisher-loc>: <publisher-name>Brooks/Cole</publisher-name>.</citation>
</ref>
<ref id="bibr10-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bensley</surname>
<given-names>D. A</given-names>
</name>
</person-group>. (<year>2009</year>). <article-title>Thinking critically about critical thinking approaches: Comment on Yancher, Slife, and Warne 2008</article-title>. <source>Review of General Psychology</source>, <volume>13</volume>, <fpage>275</fpage>–<lpage>277</lpage>. <comment>doi:10.1037/a0015654</comment>
</citation>
</ref>
<ref id="bibr11-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bensley</surname>
<given-names>D. A.</given-names>
</name>
</person-group> (<year>2010</year>). <article-title>A brief guide to teaching and assessing critical thinking skills in psychology</article-title>. <source>Observer</source>, <volume>23</volume>(<issue>10</issue>), <fpage>49</fpage>–<lpage>53</lpage>.</citation>
</ref>
<ref id="bibr12-0098628311430642">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Bensley</surname>
<given-names>D. A</given-names>
</name>
</person-group>. (<year>in press</year>). <article-title>Rules for reasoning revisited: Towards a scientific conception of critical thinking</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Horvath</surname>
<given-names>C. P.</given-names>
</name>
<name>
<surname>Forte</surname>
<given-names>J. M.</given-names>
</name>
</person-group> (Eds.), <source>Critical thinking</source>. <publisher-loc>Hauppauge, NY</publisher-loc>: <publisher-name>Nova Science</publisher-name>.</citation>
</ref>
<ref id="bibr13-0098628311430642">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Bensley</surname>
<given-names>D. A.</given-names>
</name>
<name>
<surname>Bates</surname>
<given-names>A</given-names>
</name>
</person-group>. (<year>2006</year>). <source>The inventory of thinking skills in psychology</source>. <comment>Unpublished manuscript</comment>.</citation>
</ref>
<ref id="bibr14-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bensley</surname>
<given-names>D. A.</given-names>
</name>
<name>
<surname>Crowe</surname>
<given-names>D. S.</given-names>
</name>
<name>
<surname>Bernhardt</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Buckner</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Allman</surname>
<given-names>A. L</given-names>
</name>
</person-group>. (<year>2010</year>). <article-title>Teaching and assessing critical thinking skills for argument analysis in psychology</article-title>. <source>Teaching of Psychology</source>, <volume>37</volume>, <fpage>91</fpage>–<lpage>96</lpage>. <comment>doi:10.1080/00986281003626656</comment>
</citation>
</ref>
<ref id="bibr15-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bensley</surname>
<given-names>D. A.</given-names>
</name>
<name>
<surname>Haynes</surname>
<given-names>C</given-names>
</name>
</person-group>. (<year>1995</year>). <article-title>The acquisition of general purpose strategic knowledge for argumentation</article-title>. <source>Teaching of Psychology</source>, <volume>22</volume>, <fpage>41</fpage>–<lpage>45</lpage>. <comment>doi:10.1207/s15328023top2201_13</comment>
</citation>
</ref>
<ref id="bibr16-0098628311430642">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Bensley</surname>
<given-names>D. A.</given-names>
</name>
<name>
<surname>Powell</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Flinn</surname>
<given-names>J. A.</given-names>
</name>
</person-group> (<year>2010</year>, <article-title>March)</article-title>. <source>Development of a test for critically evaluating information on the Internet</source>. <conf-name>Poster session presented at the 81st annual meeting of the Eastern Psychological Association</conf-name>, <conf-loc>Brooklyn, NY</conf-loc>.</citation>
</ref>
<ref id="bibr17-0098628311430642">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Bensley</surname>
<given-names>D. A.</given-names>
</name>
<name>
<surname>Spero</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Kennedy</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Murtagh</surname>
<given-names>M. P.</given-names>
</name>
<name>
<surname>Bernhardt</surname>
<given-names>P. S.</given-names>
</name>
</person-group> (<year>2010</year>, <article-title>May)</article-title>. <source>Individual differences in critical thinking skills, dispositions, and belief revision</source>. <conf-name>Poster session presented at the 22nd meeting of the Association for Psychological Science</conf-name>, <conf-loc>Boston, MA</conf-loc>.</citation>
</ref>
<ref id="bibr18-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bernard</surname>
<given-names>R. M.</given-names>
</name>
<name>
<surname>Zhang</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Abrami</surname>
<given-names>P. C.</given-names>
</name>
<name>
<surname>Sicoly</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Borokhovski</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Surkes</surname>
<given-names>M. A</given-names>
</name>
</person-group>. (<year>2008</year>). <article-title>Exploring the structure of the Watson-Glaser Critical Thinking Appraisal: One scale or many subscales?</article-title> <source>Thinking Skills and Creativity</source>, <volume>3</volume>, <fpage>15</fpage>–<lpage>22</lpage>. <comment>doi:10.1016/j.tsc.2007.11.001</comment>
</citation>
</ref>
<ref id="bibr19-0098628311430642">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Bernhardt</surname>
<given-names>P. C.</given-names>
</name>
<name>
<surname>Foley</surname>
<given-names>M. D.</given-names>
</name>
<name>
<surname>Evans</surname>
<given-names>H. B.</given-names>
</name>
</person-group> (<year>2010</year>, <article-title>March)</article-title>. <source>Assessment of methodological and causal reasoning skills in research methods classes</source>. <conf-name>Presented at the 81st annual meeting of the Eastern Psychological Association</conf-name>, <conf-loc>Brooklyn, NY</conf-loc>.</citation>
</ref>
<ref id="bibr20-0098628311430642">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Beyer</surname>
<given-names>B. K.</given-names>
</name>
</person-group> (<year>1997</year>). <source>Improving student thinking: A comprehensive approach</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Allyn &amp; Bacon</publisher-name>.</citation>
</ref>
<ref id="bibr21-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Black</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Wiliam</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>1998</year>). <article-title>Assessment and classroom learning</article-title>. <source>Assessment in Education</source>, <volume>5</volume>, <fpage>7</fpage>–<lpage>73</lpage>.</citation>
</ref>
<ref id="bibr22-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bollag</surname>
<given-names>B.</given-names>
</name>
</person-group> (<year>2006, October 27</year>). <article-title>Making an art form of assessment</article-title>. <source>Chronicle of Higher Education</source>, <volume>53</volume>(<issue>10</issue>), <fpage>A8</fpage>–<lpage>A10</lpage>.</citation>
</ref>
<ref id="bibr23-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cacioppo</surname>
<given-names>J. T.</given-names>
</name>
<name>
<surname>Petty</surname>
<given-names>R. E.</given-names>
</name>
<name>
<surname>Feinstein</surname>
<given-names>J. A.</given-names>
</name>
<name>
<surname>Jarvis</surname>
<given-names>W. B</given-names>
</name>
</person-group>. (<year>1996</year>). <article-title>Dispositional differences in cognitive motivation: The life and times of individuals varying in need for cognition</article-title>. <source>Psychological Bulletin</source>, <volume>119</volume>, <fpage>197</fpage>–<lpage>253</lpage>. <comment>doi:10.1037/0033–2909.119.2.197</comment>
</citation>
</ref>
<ref id="bibr24-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cacioppo</surname>
<given-names>J. T.</given-names>
</name>
<name>
<surname>Petty</surname>
<given-names>R. E.</given-names>
</name>
<name>
<surname>Kao</surname>
<given-names>C. F</given-names>
</name>
</person-group>. (<year>1984</year>). <article-title>The efficient assessment of need for cognition</article-title>. <source>Journal of Personality Assessment</source>, <volume>48</volume>, <fpage>306</fpage>–<lpage>307</lpage>. <comment>doi:10.1207/s15327752jpa4803_13</comment>
</citation>
</ref>
<ref id="bibr25-0098628311430642">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Chance</surname>
<given-names>P.</given-names>
</name>
</person-group> (<year>1986</year>). <source>Thinking in the classroom: A review of programs</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Instructors College Press</publisher-name>.</citation>
</ref>
<ref id="bibr26-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Clauser</surname>
<given-names>B. E</given-names>
</name>
</person-group>. (<year>2000</year>). <article-title>Recurrent issues and recent advances in scoring performance assessments</article-title>. <source>Applied Psychological Measurement</source>, <volume>24</volume>, <fpage>310</fpage>–<lpage>324</lpage>. <comment>doi:10.1177/01466210022031778</comment>
</citation>
</ref>
<ref id="bibr27-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Clifford</surname>
<given-names>J. S.</given-names>
</name>
<name>
<surname>Boufal</surname>
<given-names>M. M.</given-names>
</name>
<name>
<surname>Kurtz</surname>
<given-names>J. E</given-names>
</name>
</person-group>. (<year>2004</year>). <article-title>Personality traits and critical thinking skills in college students: Empirical tests of a two-factor theory</article-title>. <source>Assessment</source>, <volume>11</volume>, <fpage>169</fpage>–<lpage>176</lpage>. <comment>doi:10.1177/1073191104263250</comment>
</citation>
</ref>
<ref id="bibr28-0098628311430642">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Costa</surname>
<given-names>P. T.</given-names>
</name>
<name>
<surname>McCrae</surname>
<given-names>R. R.</given-names>
</name>
</person-group> (<year>1992</year>). <source>Revised NEO personality inventory</source>. <publisher-loc>Odessa, FL</publisher-loc>: <publisher-name>Psychological Assessment Resources</publisher-name>.</citation>
</ref>
<ref id="bibr29-0098628311430642">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Ennis</surname>
<given-names>R. H.</given-names>
</name>
</person-group> (<year>1987</year>). <article-title>A taxonomy of critical thinking dispositions and abilities</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Baron</surname>
<given-names>J. B.</given-names>
</name>
<name>
<surname>Sternberg</surname>
<given-names>R. F.</given-names>
</name>
</person-group> (Eds.). <source>Teaching thinking skills: Theory and practice</source> (pp. <fpage>9</fpage>–<lpage>26</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Freeman</publisher-name>.</citation>
</ref>
<ref id="bibr30-0098628311430642">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Facione</surname>
<given-names>P. A.</given-names>
</name>
<name>
<surname>Facione</surname>
<given-names>N. C.</given-names>
</name>
</person-group> (<year>1992</year>). <source>The California Critical Thinking Disposition Inventory</source>. <publisher-loc>Millbrae, CA</publisher-loc>: <publisher-name>California Academic Press</publisher-name>.</citation>
</ref>
<ref id="bibr31-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Follman</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Reliability estimates of contemporary critical thinking instruments</article-title>. <source>Korean Journal of Thinking &amp; Problem Solving</source>, <volume>13</volume>, <fpage>73</fpage>–<lpage>81</lpage>.</citation>
</ref>
<ref id="bibr32-0098628311430642">
<citation citation-type="journal">
<collab collab-type="author">Frostburg State University Psychology Department</collab>. <article-title>Assessment activities of the FSU Psychology Department reported for the 2009–2010 academic year</article-title>. <comment>Internal publication</comment>.</citation>
</ref>
<ref id="bibr33-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gadzella</surname>
<given-names>B. M.</given-names>
</name>
<name>
<surname>Hogan</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Masten</surname>
<given-names>W.</given-names>
</name>
<name>
<surname>Stacks</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Stephens</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Zascavage</surname>
<given-names>V.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>Reliability and validity of the Watson-Glaser critical thinking-forms for different academic groups</article-title>. <source>Journal of Instructional Psychology</source>, <volume>33</volume>, <fpage>141</fpage>–<lpage>143</lpage>.</citation>
</ref>
<ref id="bibr34-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Graham</surname>
<given-names>S. E</given-names>
</name>
</person-group>. (<year>1998</year>). <article-title>Developing student outcomes for the psychology major: An assessment-as-learning framework</article-title>. <source>Current Directions in Psychological Science</source>, <volume>7</volume>, <fpage>165</fpage>–<lpage>170</lpage>. <comment>doi:10.1111/1467–8721.ep10836897</comment>
</citation>
</ref>
<ref id="bibr35-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Halonen</surname>
<given-names>J. S</given-names>
</name>
</person-group>. (<year>1995</year>). <article-title>Demystifying critical thinking</article-title>. <source>Teaching of Psychology</source>, <volume>22</volume>, <fpage>75</fpage>–<lpage>81</lpage>. <comment>doi:10.1207/s15328023top2201_23</comment>
</citation>
</ref>
<ref id="bibr36-0098628311430642">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Halonen</surname>
<given-names>J. S.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Measure for measure: The challenge of assessing critical thinking</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Dunn</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Halonen</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Smith</surname>
<given-names>R.</given-names>
</name>
</person-group> (Eds.), <source>Teaching critical thinking in psychology: A handbook of best practices</source> (pp. <fpage>61</fpage>–<lpage>75</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Wiley-Blackwell</publisher-name>.</citation>
</ref>
<ref id="bibr37-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Halonen</surname>
<given-names>J. S.</given-names>
</name>
<name>
<surname>Bosack</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Clay</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>McCarthy</surname>
<given-names>M</given-names>
</name>
</person-group>. (<year>2003</year>). <article-title>A rubric for learning, teaching, and assessing scientific inquiry in psychology</article-title>. <source>Teaching of Psychology</source>, <volume>30</volume>, <fpage>196</fpage>–<lpage>208</lpage>. <comment>doi:10.1207/S15328023TOP3003_01</comment>
</citation>
</ref>
<ref id="bibr38-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Halpern</surname>
<given-names>D. F</given-names>
</name>
</person-group>. (<year>1988</year>). <article-title>Assessing student outcomes for psychology majors</article-title>. <source>Teaching of Psychology</source>, <volume>15</volume>, <fpage>181</fpage>–<lpage>186</lpage>. <comment>doi:10.1207/s15328023top1504_1</comment>
</citation>
</ref>
<ref id="bibr39-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Halpern</surname>
<given-names>D. F</given-names>
</name>
</person-group>. (<year>1998</year>). <article-title>Teaching critical thinking for transfer across domains: Dispositions, skills, structure training, and metacognitive monitoring</article-title>. <source>American Psychologist</source>, <volume>53</volume>, <fpage>449</fpage>–<lpage>455</lpage>. <comment>doi:10.1037/0003–066X.53.4.449</comment>
</citation>
</ref>
<ref id="bibr40-0098628311430642">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Halpern</surname>
<given-names>D. F.</given-names>
</name>
</person-group> (<year>2004</year>). <article-title>Outcomes assessment 101</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Dunn</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Mehrota</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Halonen</surname>
<given-names>J.</given-names>
</name>
</person-group> (Eds.), <source>Measuring up: Educational challenges and practices for psychology</source> (pp. <fpage>11</fpage>–<lpage>26</lpage>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>American Psychological Association</publisher-name>.</citation>
</ref>
<ref id="bibr41-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Halpern</surname>
<given-names>D. F.</given-names>
</name>
<name>
<surname>Smothergill</surname>
<given-names>D. W.</given-names>
</name>
<name>
<surname>Allen</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Baker</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Baum</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Best</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Weaver</surname>
<given-names>K</given-names>
</name>
</person-group>. (<year>1998</year>). <article-title>Scholarship in psychology: A paradigm for the twenty first century</article-title>. <source>American Psychologist</source>, <volume>53</volume>, <fpage>1292</fpage>–<lpage>1297</lpage>. <comment>doi:10.1037/0003–066X.53.12.1292</comment>
</citation>
</ref>
<ref id="bibr42-0098628311430642">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Janesick</surname>
<given-names>V. J.</given-names>
</name>
</person-group> (<year>2006</year>). <source>Authentic assessment</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Peter Lang</publisher-name>.</citation>
</ref>
<ref id="bibr43-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kruger</surname>
<given-names>D. J.</given-names>
</name>
<name>
<surname>Zechmeister</surname>
<given-names>E. B</given-names>
</name>
</person-group>. (<year>2001</year>). <article-title>A skills-experience inventory for the undergraduate psychology major</article-title>. <source>Teaching of Psychology</source>, <volume>28</volume>, <fpage>249</fpage>–<lpage>253</lpage>. <comment>doi:10.1207/S15328023TOP2804_02</comment>
</citation>
</ref>
<ref id="bibr44-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lawson</surname>
<given-names>T. J</given-names>
</name>
</person-group>. (<year>1999</year>). <article-title>Assessing psychological critical thinking as an outcome for psychology majors</article-title>. <source>Teaching of Psychology</source>, <volume>26</volume>, <fpage>207</fpage>–<lpage>209</lpage>. <comment>doi:10.1207/S15328023TOP260311</comment>
</citation>
</ref>
<ref id="bibr45-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Leary</surname>
<given-names>M. S.</given-names>
</name>
<name>
<surname>Shepperd</surname>
<given-names>J. A.</given-names>
</name>
<name>
<surname>McNeill</surname>
<given-names>M. S.</given-names>
</name>
<name>
<surname>Jenkins</surname>
<given-names>T. B.</given-names>
</name>
<name>
<surname>Barnes</surname>
<given-names>B. D</given-names>
</name>
</person-group>. (<year>1986</year>). <article-title>Objectivism in information utilization: Theory and measurement</article-title>. <source>Journal of Personality Assessment</source>, <volume>50</volume>, <fpage>32</fpage>–<lpage>43</lpage>. <comment>doi:10.1207/s15327752jpa5001_5</comment>
</citation>
</ref>
<ref id="bibr46-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lehman</surname>
<given-names>D. R.</given-names>
</name>
<name>
<surname>Nisbett</surname>
<given-names>R. E.</given-names>
</name>
</person-group> (<year>1990</year>). <article-title>A longitudinal study of the effects of undergraduate training on reasoning</article-title>. <source>Developmental Psychology</source>, <volume>26</volume>, <fpage>952</fpage>–<lpage>960</lpage>.</citation>
</ref>
<ref id="bibr47-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Leighton</surname>
<given-names>J. P.</given-names>
</name>
<name>
<surname>Gierl</surname>
<given-names>M. J</given-names>
</name>
</person-group>. (<year>2007</year>). <article-title>Defining and evaluating models of cognition used in educational measurement to make inferences about examinees’ thinking processes</article-title>. <source>Educational Measures: Issues and Practices</source>, <volume>26</volume>, <fpage>3</fpage>–<lpage>16</lpage>. <comment>doi:10.1111/j.1745–3992.2007.00090.x</comment>
</citation>
</ref>
<ref id="bibr48-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Levy</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Burton</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Mickler</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Vigorito</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>1999</year>). <article-title>A curriculum matrix for psychology program review</article-title>. <source>Teaching of Psychology</source>, <volume>26</volume>, <fpage>291</fpage>–<lpage>294</lpage>.</citation>
</ref>
<ref id="bibr49-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>LoSchiavo</surname>
<given-names>F. M.</given-names>
</name>
<name>
<surname>Shatz</surname>
<given-names>M. A.</given-names>
</name>
<name>
<surname>Poling</surname>
<given-names>D. A</given-names>
</name>
</person-group>. (<year>2008</year>). <article-title>Strengthening the scholarship of teaching and learning via experimentation</article-title>. <source>Teaching of Psychology</source>, <volume>35</volume>, <fpage>301</fpage>–<lpage>304</lpage>. <comment>doi:10.1080/00986280802377164</comment>
</citation>
</ref>
<ref id="bibr50-0098628311430642">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Maki</surname>
<given-names>P. L.</given-names>
</name>
</person-group> (<year>2004</year>). <source>Assessing for learning: Building a sustainable commitment across the institution</source>. <publisher-loc>Sterling, VA</publisher-loc>: <publisher-name>Stylus</publisher-name>.</citation>
</ref>
<ref id="bibr51-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>McCarthy</surname>
<given-names>M. A.</given-names>
</name>
<name>
<surname>Niederjohn</surname>
<given-names>D. M.</given-names>
</name>
<name>
<surname>Bosack</surname>
<given-names>T. N</given-names>
</name>
</person-group>, (<year>2011</year>). <article-title>Embedded assessment: A measure of student learning and teaching effectiveness</article-title>. <source>Teaching of Psychology</source>, <volume>38</volume>, <fpage>78</fpage>–<lpage>82</lpage>. <comment>doi:10.1177/0098628311401590</comment>
</citation>
</ref>
<ref id="bibr52-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>McLean</surname>
<given-names>C. P.</given-names>
</name>
<name>
<surname>Miller</surname>
<given-names>N. A</given-names>
</name>
</person-group>. (<year>2010</year>). <article-title>Changes in critical thinking skills following a course on science and pseudoscience: A quasi-experimental study</article-title>. <source>Teaching of Psychology</source>, <volume>37</volume>, <fpage>85</fpage>–<lpage>90</lpage>. <comment>doi:10.1080/00986281003626714</comment>
</citation>
</ref>
<ref id="bibr53-0098628311430642">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>McPeck</surname>
<given-names>J. E.</given-names>
</name>
</person-group> (<year>1981</year>). <source>Critical thinking and education</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>St. Martin’s Press</publisher-name>.</citation>
</ref>
<ref id="bibr54-0098628311430642">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Mentkowski</surname>
<given-names>M.</given-names>
</name>
</person-group>
<collab collab-type="author">Associates</collab>. (<year>2000</year>). <source>Learning that lasts: Integrating learning, performance, and development in college and beyond</source>. <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation>
</ref>
<ref id="bibr55-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nieto</surname>
<given-names>A. M.</given-names>
</name>
<name>
<surname>Saiz</surname>
<given-names>C.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Evaluation of Halpern’s “structural component” for improving critical thinking</article-title>. <source>The Spanish Journal of Psychology</source>, <volume>11</volume>(<issue>1</issue>), <fpage>266</fpage>–<lpage>274</lpage>.</citation>
</ref>
<ref id="bibr56-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pacini</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Epstein</surname>
<given-names>S</given-names>
</name>
</person-group>. (<year>1999</year>). <article-title>The relation of rational and experiential information processing styles to personality, basic beliefs, and the ratio-bias phenomenon</article-title>. <source>Journal of Personality and Social Psychology</source>, <volume>76</volume>, <fpage>972</fpage>–<lpage>987</lpage>. <comment>doi:10.1037/0022–3514.76.6.972</comment>
</citation>
</ref>
<ref id="bibr57-0098628311430642">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Parsons</surname>
<given-names>E. A.</given-names>
</name>
<name>
<surname>Powell</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Bensley</surname>
<given-names>D. A.</given-names>
</name>
<name>
<surname>Crowe</surname>
<given-names>D. S.</given-names>
</name>
</person-group> (<year>2010</year>, <article-title>March)</article-title>. <source>Reliability and validity of the inventory of thinking dispositions in psychology</source>. <conf-name>Poster session presented at the 81st annual meeting of the Eastern Psychological Association</conf-name>, <conf-loc>Brooklyn, NY</conf-loc>.</citation>
</ref>
<ref id="bibr58-0098628311430642">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Pascarella</surname>
<given-names>E. T.</given-names>
</name>
<name>
<surname>Terrenzini</surname>
<given-names>P. T.</given-names>
</name>
</person-group> (<year>1991</year>). <source>How college affects students: Findings and insights from twenty years of research</source>. <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation>
</ref>
<ref id="bibr59-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Penningroth</surname>
<given-names>S. L.</given-names>
</name>
<name>
<surname>Despain</surname>
<given-names>L. H.</given-names>
</name>
<name>
<surname>Gray</surname>
<given-names>M. J.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>A course designed to improve psychological critical thinking</article-title>. <source>Teaching of Psychology</source>, <volume>34</volume>, <fpage>153</fpage>–<lpage>157</lpage>.</citation>
</ref>
<ref id="bibr60-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pieper</surname>
<given-names>S. L.</given-names>
</name>
<name>
<surname>Fulcher</surname>
<given-names>K. H.</given-names>
</name>
<name>
<surname>&amp; Sundre</surname>
<given-names>D. L.</given-names>
</name>
<name>
<surname>Erwin</surname>
<given-names>T. D</given-names>
</name>
</person-group>. (<year>2008</year>). <article-title>“What do I do with the data now?” Analyzing assessment information for accountability and improvement</article-title>. <source>Research &amp; Practice in Assessment</source>, <volume>2</volume>, <fpage>1</fpage>–<lpage>8</lpage>.</citation>
</ref>
<ref id="bibr61-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Possin</surname>
<given-names>K.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>A field guide to critical-thinking assessment</article-title>. <source>Teaching philosophy</source>, <volume>31</volume>, <fpage>201</fpage>–<lpage>228</lpage>.</citation>
</ref>
<ref id="bibr62-0098628311430642">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Pusateri</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Halonen</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Hill</surname>
<given-names>W.</given-names>
</name>
<name>
<surname>McCarthy</surname>
<given-names>M</given-names>
</name>
</person-group>. (<year>2009</year>). <source>The assessment cyberguide for learning goals and outcomes</source> (<edition>2nd ed</edition>.). <comment>American Psychological Association Education Directorate. Retrieved from</comment> <ext-link ext-link-type="uri" xlink:href="http://www.apa.org/edgovernance/bea/assessment-cyberguide-v2.pdf">http://www.apa.org/edgovernance/bea/assessment-cyberguide-v2.pdf</ext-link>
</citation>
</ref>
<ref id="bibr63-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rogers</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Mentkowski</surname>
<given-names>M</given-names>
</name>
</person-group>. (<year>2004</year>). <article-title>Abilities that distinguish the effectiveness of five-year alumna performance across work, family and civic roles: A higher education validation</article-title>. <source>Higher Education Research &amp; Development</source>, <volume>23</volume>, <fpage>347</fpage>–<lpage>374</lpage>. <comment>doi:10.1080/0729436042000235445</comment>
</citation>
</ref>
<ref id="bibr64-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Smith</surname>
<given-names>R. A.</given-names>
</name>
<name>
<surname>Buskist</surname>
<given-names>W</given-names>
</name>
</person-group>. (<year>2008</year>). <article-title>The scholarship of teaching and learning in psychology</article-title>. <source>Teaching of Psychology</source>, <volume>35</volume>, <fpage>247</fpage>–<lpage>248</lpage>. <comment>doi:10.1080/00986280802418737</comment>
</citation>
</ref>
<ref id="bibr65-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Solon</surname>
<given-names>T.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>Generic critical thinking infusion and course content learning in introductory psychology</article-title>. <source>Journal of Instructional Psychology</source>, <volume>34</volume>, <fpage>972</fpage>–<lpage>987</lpage>.</citation>
</ref>
<ref id="bibr66-0098628311430642">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Spero</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Bensley</surname>
<given-names>D. A.</given-names>
</name>
<name>
<surname>Kennedy</surname>
<given-names>E</given-names>
</name>
</person-group>. (<year>2010</year>, <article-title>March)</article-title>. <source>Critical thinking skills, dispositions, and belief revision in a critical reading task</source>. <conf-name>Poster session presented at the 81st annual meeting of the Eastern Psychological Association</conf-name>, <conf-loc>Brooklyn, NY</conf-loc>.</citation>
</ref>
<ref id="bibr67-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Stanovich</surname>
<given-names>K. E.</given-names>
</name>
<name>
<surname>West</surname>
<given-names>R. F</given-names>
</name>
</person-group>. (<year>1997</year>). <article-title>Reasoning independently of prior belief and individual differences in actively open-minded thinking</article-title>. <source>Journal of Educational Psychology</source>, <volume>89</volume>, <fpage>342</fpage>–<lpage>357</lpage>. <comment>doi:10.1037/0022–0663.89.2.342</comment>
</citation>
</ref>
<ref id="bibr68-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Stellmack</surname>
<given-names>M. A.</given-names>
</name>
<name>
<surname>Konheim-Kalkstein</surname>
<given-names>Y. L.</given-names>
</name>
<name>
<surname>Manor</surname>
<given-names>J. E.</given-names>
</name>
<name>
<surname>Massey</surname>
<given-names>A. R.</given-names>
</name>
<name>
<surname>Schmitz</surname>
<given-names>J. A</given-names>
</name>
</person-group>. (<year>2009</year>). <article-title>An assessment of reliability and validity of a rubric for grading APA-style introductions</article-title>. <source>Teaching of Psychology</source>, <volume>36</volume>, <fpage>102</fpage>–<lpage>107</lpage>. <comment>doi:10.1080/00986280902739776</comment>
</citation>
</ref>
<ref id="bibr69-0098628311430642">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Stoloff</surname>
<given-names>M. L.</given-names>
</name>
<name>
<surname>Apple</surname>
<given-names>K. J.</given-names>
</name>
<name>
<surname>Barron</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Reis-Bergan</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Sundre</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>2004</year>). <article-title>Seven goals for effective program assessment</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Dunn</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Mehrota</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Halonen</surname>
<given-names>J.</given-names>
</name>
</person-group>, (Eds.), <source>Measuring up: Educational challenges and practices for psychology</source> (pp. <fpage>29</fpage>–<lpage>46</lpage>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>American Psychological Association</publisher-name>.</citation>
</ref>
<ref id="bibr70-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sundre</surname>
<given-names>D. L.</given-names>
</name>
<name>
<surname>Moore</surname>
<given-names>D. L.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>The student opinion scale: A measure of examinee motivation</article-title>. <source>Assessment Update</source>, <volume>14</volume>, <fpage>8</fpage>–<lpage>9</lpage>.</citation>
</ref>
<ref id="bibr71-0098628311430642">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Tarricone</surname>
<given-names>P.</given-names>
</name>
</person-group> (<year>2011</year>). <source>The taxonomy of metacogniton</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Psychology Press</publisher-name>.</citation>
</ref>
<ref id="bibr72-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Taube</surname>
<given-names>K. T.</given-names>
</name>
</person-group> (<year>1997</year>). <article-title>Critical thinking ability and disposition as factors of performance on a written critical thinking test</article-title>. <source>Journal of General Education</source>, <volume>46</volume>, <fpage>129</fpage>–<lpage>164</lpage>.</citation>
</ref>
<ref id="bibr73-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Thaler</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Kazemi</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Huscher</surname>
<given-names>C</given-names>
</name>
</person-group>. (<year>2009</year>). <article-title>Developing a rubric to assess student learning outcomes using a class assignment</article-title>. <source>Teaching of Psychology</source>, <volume>36</volume>, <fpage>113</fpage>–<lpage>116</lpage>. <comment>doi:10.1080/00986280902739305</comment>
</citation>
</ref>
<ref id="bibr74-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Toplak</surname>
<given-names>M. E.</given-names>
</name>
<name>
<surname>Stanovich</surname>
<given-names>K. E</given-names>
</name>
</person-group>. (<year>2002</year>). <article-title>The domain specificity and generality of disjunctive reasoning: Searching for a generalizable critical thinking skill</article-title>. <source>Journal of Educational Psychology</source>, <volume>94</volume>, <fpage>197</fpage>–<lpage>209</lpage>. <comment>doi:10.1037/0022–0663.94.1.197</comment>
</citation>
</ref>
<ref id="bibr75-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Williams</surname>
<given-names>R. L</given-names>
</name>
</person-group>. (<year>1999</year>). <article-title>Operational definitions and assessment of higher-order cognitive constructs</article-title>. <source>Educational Psychology Review</source>, <volume>11</volume>, <fpage>411</fpage>–<lpage>427</lpage>. <comment>doi:10.1023/A:1022065517997</comment>
</citation>
</ref>
<ref id="bibr76-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Williams</surname>
<given-names>R. L.</given-names>
</name>
<name>
<surname>Oliver</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Allin</surname>
<given-names>J. L.</given-names>
</name>
<name>
<surname>Winn</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Booher</surname>
<given-names>C. S</given-names>
</name>
</person-group>. (<year>2003</year>). <article-title>Psychological critical thinking as a course predictor and outcome variable</article-title>. <source>Teaching of Psychology</source>, <volume>30</volume>, <fpage>220</fpage>–<lpage>223</lpage>. <comment>doi:10.1207/S15328023TOP3003_04</comment>
</citation>
</ref>
<ref id="bibr77-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wolf</surname>
<given-names>L. F.</given-names>
</name>
<name>
<surname>Smith</surname>
<given-names>J. K.</given-names>
</name>
</person-group> (<year>1995</year>). <article-title>The consequence of consequence: Motivation, anxiety, and test performance</article-title>. <source>Applied Measurement in Education</source>, <volume>8</volume>, <fpage>227</fpage>–<lpage>242</lpage>.</citation>
</ref>
<ref id="bibr78-0098628311430642">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yanchar</surname>
<given-names>S. C.</given-names>
</name>
<name>
<surname>Slife</surname>
<given-names>B. D.</given-names>
</name>
<name>
<surname>Warne</surname>
<given-names>R.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Critical thinking as disciplinary practice</article-title>. <source>Review of General Psychology</source>, <volume>12</volume>, <fpage>265</fpage>–<lpage>281</lpage>. <comment>doi:10.1037/1089–2680.12.3.265</comment>.</citation>
</ref>
</ref-list>
</back>
</article>